{% extends 'base_generic.html'%}

{% block title %}
Visualization
{% endblock %}

{% block content %}
    <style>
        img{ 
            width:100%;
            margin:15px;
           }
    </style>
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12">
                <h1 class='mt-2'>Shap explainers of my XGBoost model</h1>
                <hr class='mt-0 mb-4'>

    <div class='col-12 text-center'>
        <a href="{% url 'data'%}">
            <button class="btn bg-white" style="color:black;">
                Download csv?
            </button>
        </a>
    </div>
                {% load static %}
    <!--                     
                {% for file in image %}
                {{file}}
                    
                    <img src="{% static '/image/{{ file }}' %}" />
                    
                {% endfor %} -->
                
                <img class='image' src="{% static 'image/initial.png' %}"/>
                <p style="color:white;">The plot below is called a force plot. It shows features contributing to push the prediction from the base value. The base value is the average model output over the training dataset we passed. Features pushing the prediction higher are shown in red. Features pushing it lower appear in blue.</p>
                <div class="row">
                    <div class="col-6">
                        <img src="{% static 'image/barplot.png' %}" alt="" />
                        <p style="color:white;">Variable importance graphs are useful tools for understanding the model in a global sense. SHAP provides a theoretically sound method for evaluating variable importance. This is important, given the debate over which of the traditional methods of calculating variable importance is correct and that those methods do not always agree.</p>
                    </div>
                    <div class="col-6">
                        <img src="{% static 'image/shap_value.png' %}"/>
                        <p style="color:white;">Similar to a variable importance plot, SHAP also offers a summary plot showing the SHAP values for every instance from the training dataset. This can lead to a better understanding of overall patterns and allow discovery of pockets of prediction outliers.</p>
                    </div>    
                </div>
                <div class="row">
                    <div class="col-12">
                        <img src="{% static 'image/dependance.png' %}"/>
                        <p style="color:white;">Variable influence or dependency plots have long been a favorite of statisticians for model interpretability. SHAP provides these as well, and I find them quite useful.
                        </p>
                    </div>
                    
                    
                </div>
                <div class="row">
                    <div class="col-6">
                        <img src="{% static 'image/tree.png' %}"/>
                        <p style="color:white;">Plot importance based on fitted trees.</p>
                    </div>
                    <div class="col-6">
                        <img src="{% static 'image/importance.png' %}"/>
                        <p style="color:white;">Plot specified tree.</p>
                    </div>
                    
                </div>
                
                
            </div>
        </div>
    </div>

{% endblock %}
